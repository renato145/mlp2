{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering - NYTimes news articles\n",
    "    299752 documents \n",
    "    101636 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws =''\n",
    "for p in os.getcwd().split('/')[:-1]: ws+=p+'/' \n",
    "words = pd.read_csv(ws + 'DATA/nytimes/vocab.nytimes.txt', header=None)\n",
    "words.columns = ['word']\n",
    "\n",
    "count_data = pd.read_csv(ws + 'DATA/nytimes/docword.nytimes.txt', sep=' ', skiprows=3,\n",
    "                 header=None)\n",
    "count_data.columns = ['docID','wordID','count']\n",
    "count_data = count_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build count sparse matrix.\n",
    "* Using Compressed Sparse Row format matrix (CSR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, row_pos = np.unique(count_data[:, 0], return_inverse=True)\n",
    "cols, col_pos = np.unique(count_data[:, 1], return_inverse=True)\n",
    "pivot_table = sps.coo_matrix((count_data[:, 2], (row_pos, col_pos)),\n",
    "                             shape=(len(rows), len(cols)))\n",
    "pivot_table = pivot_table.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform to Tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSA + K-means Clustering\n",
    "* Find the best number of components and clusters using silhouette score.\n",
    "* Mini batch k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 010, clusters: 05, silhouette score: 0.2976 (29 seconds)\n",
      "Components: 010, clusters: 06, silhouette score: 0.2743 (28 seconds)\n",
      "Components: 010, clusters: 07, silhouette score: 0.3078 (29 seconds)\n",
      "Components: 010, clusters: 08, silhouette score: 0.3075 (29 seconds)\n",
      "Components: 010, clusters: 09, silhouette score: 0.3034 (28 seconds)\n",
      "Components: 010, clusters: 10, silhouette score: 0.3176 (30 seconds)\n",
      "Components: 010, clusters: 11, silhouette score: 0.2629 (30 seconds)\n",
      "Components: 010, clusters: 12, silhouette score: 0.2421 (28 seconds)\n",
      "Components: 010, clusters: 13, silhouette score: 0.2740 (30 seconds)\n",
      "Components: 010, clusters: 14, silhouette score: 0.2715 (29 seconds)\n",
      "Components: 010, clusters: 15, silhouette score: 0.2650 (30 seconds)\n",
      "\n",
      "Components: 015, clusters: 05, silhouette score: 0.2037 (43 seconds)\n",
      "Components: 015, clusters: 06, silhouette score: 0.2008 (43 seconds)\n",
      "Components: 015, clusters: 07, silhouette score: 0.2352 (43 seconds)\n",
      "Components: 015, clusters: 08, silhouette score: 0.2305 (45 seconds)\n",
      "Components: 015, clusters: 09, silhouette score: 0.2457 (44 seconds)\n",
      "Components: 015, clusters: 10, silhouette score: 0.2356 (43 seconds)\n",
      "Components: 015, clusters: 11, silhouette score: 0.2445 (43 seconds)\n",
      "Components: 015, clusters: 12, silhouette score: 0.2691 (45 seconds)\n",
      "Components: 015, clusters: 13, silhouette score: 0.2614 (44 seconds)\n",
      "Components: 015, clusters: 14, silhouette score: 0.2680 (43 seconds)\n",
      "Components: 015, clusters: 15, silhouette score: 0.2457 (44 seconds)\n",
      "\n",
      "Components: 020, clusters: 05, silhouette score: 0.1496 (47 seconds)\n",
      "Components: 020, clusters: 06, silhouette score: 0.1794 (46 seconds)\n",
      "Components: 020, clusters: 07, silhouette score: 0.1813 (47 seconds)\n",
      "Components: 020, clusters: 08, silhouette score: 0.1691 (46 seconds)\n",
      "Components: 020, clusters: 09, silhouette score: 0.2044 (47 seconds)\n",
      "Components: 020, clusters: 10, silhouette score: 0.2239 (47 seconds)\n",
      "Components: 020, clusters: 11, silhouette score: 0.2162 (48 seconds)\n",
      "Components: 020, clusters: 12, silhouette score: 0.2017 (47 seconds)\n",
      "Components: 020, clusters: 13, silhouette score: 0.2312 (48 seconds)\n",
      "Components: 020, clusters: 14, silhouette score: 0.2441 (46 seconds)\n",
      "Components: 020, clusters: 15, silhouette score: 0.2100 (46 seconds)\n",
      "\n",
      "\n",
      "Best number of components: 10\n",
      "Best number of clusters: 10\n",
      "Achieved silhouette score: 0.3176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_clusters = range(5,16,1)\n",
    "test_n_components = [10,15,20]\n",
    "normalizer = Normalizer(copy=False)\n",
    "\n",
    "best_score = 0\n",
    "for n_components in test_n_components:\n",
    "    time0 = time()\n",
    "    svd = TruncatedSVD(n_components)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X = lsa.fit_transform(tfidf)\n",
    "    elapsed_lsa = time() - time0\n",
    "    for clusters in test_clusters:\n",
    "        time0 = time()\n",
    "        km = MiniBatchKMeans(n_clusters=clusters)\n",
    "        km.fit(X)\n",
    "        elapsed_km = time() - time0\n",
    "        score = metrics.silhouette_score(X, km.labels_, sample_size=1000)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_components = n_components\n",
    "            best_n_clusters = clusters\n",
    "            best_svd = svd\n",
    "            best_km = km\n",
    "            best_X = X\n",
    "        print(\"Components: %.3d, clusters: %.2d, silhouette score: %.4f (%d seconds)\"\n",
    "          % (n_components, clusters, score, elapsed_lsa+elapsed_km))\n",
    "    print()\n",
    "\n",
    "print(\"\\nBest number of components: %i\\nBest number of clusters: %i\\nAchieved silhouette score: %.4f\"\n",
    "      % (best_n_components, best_n_clusters, best_score))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Most representative words on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most representative words for each cluster/label (K-means)\n",
      "\n",
      "Cluster 1: columnas, schlesinger, informacion, boogie, strumming, wearer, wobbled, queenly, childless, dahlias.\n",
      "\n",
      "Cluster 2: tba, gambler, search, platooned, gamely, platform, cnn, podium, zzz_knight_ridder, rumbling.\n",
      "\n",
      "Cluster 3: zzz_budroyale, tater, bilked, zzz_voila, zzz_comedy, preservationist, administration, zzz_saul_hansell, governance, customized.\n",
      "\n",
      "Cluster 4: palaver, zzz_interdenominational, zzz_interior_design, zzz_whitey, isolationism, isolationist, payola, zzz_olindo, zzz_vincent_cianci_jr, attachment.\n",
      "\n",
      "Cluster 5: schlesinger, strumming, lavish, courses, offensively, childless, governance, cascaded, drubbing, feckless.\n",
      "\n",
      "Cluster 6: companion, companeros, zzz_mesopotamia, millennial, zzz_ellen_rimbauer, stinking, marionettes, computational, zzz_immediate, busiest.\n",
      "\n",
      "Cluster 7: fillies, shot, boogie, mouthing, homage, familiarizing, wobbled, mused, longshot, captures.\n",
      "\n",
      "Cluster 8: zzz_steven_bratman, zzz_aetna, offensively, militar, zzz_budroyale, attachment, zzz_tomcat, zzz_transportation_security_act, governance, terrier.\n",
      "\n",
      "Cluster 9: pepperoni, stinking, marionettes, companion, companeros, millennial, economist, billiard, quantitative, investigative.\n",
      "\n",
      "Cluster 10: zzz_al_ahram_center, zzz_gary_scelzi, camouflage, electability, zzz_budroyale, preservationist, voluntarily, reproach, volumes, zzz_randy_myer.\n"
     ]
    }
   ],
   "source": [
    "n_words = 10\n",
    "print(\"The %d most representative words for each cluster/label (K-means)\" % n_words)\n",
    "original_space_centroids = best_svd.inverse_transform(best_km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "for i in range(best_n_clusters):\n",
    "    print(\"\\nCluster %d: \" % (i+1), end='')\n",
    "    for j,ind in enumerate(order_centroids[i, :n_words-1]):\n",
    "        print(\"%s, \" % words.values[ind,0], end='')\n",
    "    print(\"%s.\" % words.values[order_centroids[i,n_words-1],0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
